{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple demonstration of discrete Markov processes\n",
    "\n",
    "Toni Giorgino (IBF-CNR)\n",
    "\n",
    "You should run this notebook in an R instance or Google Colab. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/giorginolab/Markov-Tutorial-UniPd-2022/blob/main/R_Markov.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Discrete Markov processes\n",
    "\n",
    "A (discrete-time) Markov chain (DTMC) is a sequence of random variables, known as a stochastic process, in which the value of the next variable depends only on the value of the current variable, and not any variables in the past.\n",
    "\n",
    "At any given time the system is in one (and only one) of several *states*. We'll deal\n",
    "with *homogeneous* MC, so transition between states are governed by constant probabilities which only depend on the current state (memorylessness, or Markovian property).\n",
    "\n",
    "We copy the matrices encoding two examples seen during the lecture. \n",
    "\n",
    "For the sake of illustration, we work with the first (\"weather\") example.\n",
    "\n",
    "![weather](img/weather.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "oFqbBNMf6iCP",
    "outputId": "5b59b6a8-faa1-4ff1-a281-4b18240536c5"
   },
   "outputs": [],
   "source": [
    "# The weather example\n",
    "weatherP <- matrix( c(0.6,  0.3,  0.1,\n",
    "                      0.2,  0.3,  0.5,\n",
    "                      0.4,  0.1,  0.5  ), \n",
    "                    nrow = 3, byrow = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one with the \"colored blocks\" trajectory example was..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "oFqbBNMf6iCP",
    "outputId": "5b59b6a8-faa1-4ff1-a281-4b18240536c5"
   },
   "outputs": [],
   "source": [
    "colortrajP <- matrix( c(3/5,  0,  2/5,\n",
    "                        1/4, 3/4, 0,\n",
    "                        1/4, 2/4, 1/4  ), \n",
    "                      nrow = 3, byrow = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFLF8tTG6jcj"
   },
   "source": [
    "Note how the rows sum to unity (conservation of probability): the state must\n",
    "go somewhere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFLF8tTG6jcj"
   },
   "source": [
    "## Sampling the chain\n",
    "\n",
    "The simplest thing we can do is *sample* the chain, i.e.\n",
    "follow the probabilities to obtain one realization (here for 30 steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a Markov chain \n",
    "sample.markov <- function(P, n, s0=1) {\n",
    "        N <- nrow(P)             # number of states\n",
    "        s <- s0                  # set state <- requested initial state\n",
    "        ret <- s                 # initialize the output sequence\n",
    "        for (i in 2:n) {         # for the requested number of steps\n",
    "                p <- P[s, ]      #   take the destination probabilities\n",
    "                nx <- sample(1:N, size=1, prob=p)\n",
    "                                 #   extract the next state with those probabilities\n",
    "                ret <- c(ret, nx)#   record the new state\n",
    "                s <- nx          #   current state <- next state\n",
    "        }\n",
    "        return(ret)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P <- weatherP\n",
    "# Extract 30 time steps\n",
    "sample.markov(P, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Stationary state\n",
    "\n",
    "\n",
    "Question: if we play the game a very long time, what fraction of time would we spend in each state?\n",
    "\n",
    "* That is the asymptotic (equilibrium) distribution.\n",
    "* I.e., the probability of finding the system in a given state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the stationary state by extracting 1000 samples\n",
    "N <- 1000\n",
    "sN <- sample.markov(P, N)\n",
    "print(table(sN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is just an approximate, compute-intensive \n",
    "realization. The power of the Markov formalism lies\n",
    "in the fact that such quantities can be computed from\n",
    "the matrix *P* right away. For example,\n",
    "the stationary distribution is computed\n",
    "from the first eigenvector (see function `sstate`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove imaginary part if small\n",
    "dropSmallImaginary <- function(x) {\n",
    "        if (all(Im(z <- zapsmall(x))==0)) as.numeric(z) else x\n",
    "}\n",
    "\n",
    "# Extract stationary state as eigenvector of eigenvalue=1\n",
    "sstate <- function(M) {\n",
    "        ev<-eigen(t(M))   # compute eigenvectors and eigenvalues\n",
    "        if(dropSmallImaginary(ev$values[1]) != 1) \n",
    "                stop(\"First EV not 1\")\n",
    "        # normalize the first ev so that its elements sum to 1\n",
    "        ss <- ev$vectors[,1] / sum(ev$vectors[,1])\n",
    "        dropSmallImaginary(ss)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss <- sstate(P)\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact estimate approximates well the samples.\n",
    "We represent both graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(table(sN), xlab=\"State\", ylab=\"Times visited\")\n",
    "# Compare with the theory\n",
    "points(1:3, N*ss, col=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Stationary probabilities can be converted readily\n",
    "into free energy differences (ΔG), up to an\n",
    "irrelevant additive constant. Assuming *T* = 298 K,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kB <- 0.001985875    # kcal/mol/K\n",
    "dG <- -kB * 298 * log(ss)\n",
    "dG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "in units of kcal/mol. It is convenient to set the lowest-energy \n",
    "state (the most stable) to 0 kcal/mol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dG-min(dG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Recover the transition count matrix from a sequence\n",
    "\n",
    "Here we do the reverse than before: instead of\n",
    "using the transition probability matrix to sample a sequence,\n",
    "we now use a sequence to recover the transition count matrix\n",
    "(later I'll normalize it to get a transition *probability* matrix).\n",
    "\n",
    "Technically, I use a trick: I prepare a data frame with one \n",
    "column (I) holding the sampled values, and another\n",
    "(J) holding the same values lagged by  1 time\n",
    "step. So, I can just count the I,J pairs (with the\n",
    "`table` function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"zoo\")\n",
    "library(zoo)\n",
    "dij <- data.frame(i = sN,\n",
    "                  j = lag(zoo(sN), 1, na.pad=T))\n",
    "dij <- na.omit(dij)  # The very last transition is not valid\n",
    "T <- table(dij)\n",
    "addmargins(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now normalize by rows. Note how close it is to the\n",
    "original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estP <- T/rowSums(T)\n",
    "estP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Markovianity\n",
    "\n",
    "The sequence of states that we sampled is markovian\n",
    "**by construction**. However, in general (for real-world timeseries)\n",
    "this is not a given.\n",
    "\n",
    "To test markovianity, I will counts the transitions I → J but\n",
    "this time considering separately whether they were preceded by \n",
    "K = 1, 2 or 3. In other words, I will count  occurrences of these\n",
    "sequences: 1 → I → J,  2 → I → J, and 3 → I → J.\n",
    "\n",
    "Remember that the Markov property reads:\n",
    "\n",
    "$$ p(s_t | s_{t-1}, s_{t-2}, \\dots ) = p(s_t | s_{t-1} ) $$\n",
    "in our case, J is the destination state $s_t$, I is $s_{t-1}$,\n",
    "and the preceeding state K is $s_{t-2}$.\n",
    "\n",
    "As above, I use lags and tabulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dkij <- data.frame(k = sN,\n",
    "                   i = lag(zoo(sN), 1, na.pad=T),\n",
    "                   j = lag(zoo(sN), 2, na.pad=T))\n",
    "dkij <- na.omit(dkij)  # The very last transition is not valid\n",
    "head(dkij)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and so on.\n",
    "\n",
    "Probabilities should be independent from K. They will eventually be, \n",
    "if we sample long enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T3<-table(dkij)\n",
    "\n",
    "# Sequences 1-I-J, i.e. K=1\n",
    "print(T3[1,,]/rowSums(T3[1,,]))\n",
    "\n",
    "# Sequences 2-I-J, i.e. K=2\n",
    "print(T3[2,,]/rowSums(T3[2,,]))\n",
    "\n",
    "# Sequences 3-I-J, i.e. K=3\n",
    "print(T3[3,,]/rowSums(T3[3,,]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The probabilistic point of view\n",
    "\n",
    "Markov chains can also be followed stochastically,\n",
    "propagating the probability density. Again you will realize\n",
    "that (under suitable conditions usually met), regardless \n",
    "of the initial state, the final probability distribution\n",
    "converges towards the equilibrium one.\n",
    "\n",
    "Of note, the \"memory\" of the initial state is lost after\n",
    "a transient (related to the second eigenvalue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute time-dependent probabilities of a Markov chain. Same\n",
    "# as taking the n-th power of the matrix P. s0 is the initial\n",
    "# probability distribution over states (must sum to 1). If not\n",
    "# given, the system is assumed to start from state 1.\n",
    "probs.markov <- function(P, n, s0 = c(1,rep(0,nrow(P)-1))) {\n",
    "        N <- nrow(P)\n",
    "        s <- s0\n",
    "        ret <- s\n",
    "        for (i in 2:n) {\n",
    "                # Propagate probability distribution by multiplying\n",
    "                # on the right by matrix P for as many steps as requested.\n",
    "                s <- s %*% P\n",
    "                ret <- rbind(ret, s)\n",
    "        }\n",
    "        return(ret)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pT <- probs.markov(P, 8)\n",
    "matplot(pT, type=\"o\", \n",
    "        xlim = c(1,10),\n",
    "        xlab=\"Time (starting from T=1)\", \n",
    "        ylab=\"Probability of each state\",\n",
    "        main=\"Starting from state 1\")\n",
    "points(rep(10,3),ss,col=1:3)\n",
    "\n",
    "pT <- probs.markov(P, 8, c(0,1,0))\n",
    "matplot(pT, type=\"o\", \n",
    "        xlim = c(1,10),\n",
    "        xlab=\"Time (starting from T=1)\", \n",
    "        ylab=\"Probability of each state\",\n",
    "        main=\"Starting from state 3\")\n",
    "points(rep(10,3),ss,col=1:3)\n",
    "\n",
    "pT <- probs.markov(P, 8, c(0,0,1))\n",
    "matplot(pT, type=\"o\", \n",
    "        xlim = c(1,10),\n",
    "        xlab=\"Time (starting from T=1)\", \n",
    "        ylab=\"Probability of each state\",\n",
    "        main=\"Starting from state 3\")\n",
    "points(rep(10,3),ss,col=1:3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
